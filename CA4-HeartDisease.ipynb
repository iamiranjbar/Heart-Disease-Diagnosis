{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Heart Disease Detection (Decision tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"heart.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desicion Tree (with sklearn)\n",
    "In the first part of this project, we build a decision tree with the help of sklearn library. <br>\n",
    "In this project, we have patients features and the target is the patient have heart disease or not. <br>\n",
    "We build a decision tree with all of features and train it by 70 percent of data and test this solution with the other 30 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data\n",
    "At first, we read the csv file with __pandas read_csv__ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_data(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "csv_path = \"./heart.csv\"\n",
    "heart_disease = read_data(csv_path)\n",
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting set to data & test\n",
    "We should have to sets. One for training our model and one for testing our model. The reason for this partitioning is avoding overfitting on train data and have a good model for unseen data. <br>\n",
    "Our train data is 70 percent and the other 30 percent is for test data. We do this splitting by __train_test_split__ of __sklearn__ library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "      <td>199</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>219</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>138</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "124   39    0   2        94   199    0        1      179      0      0.0   \n",
       "72    29    1   1       130   204    0        0      202      0      0.0   \n",
       "15    50    0   2       120   219    0        1      158      0      1.6   \n",
       "10    54    1   0       140   239    0        1      160      0      1.2   \n",
       "163   38    1   2       138   175    0        1      173      0      0.0   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "124      2   0     2       1  \n",
       "72       2   0     2       1  \n",
       "15       1   0     2       1  \n",
       "10       2   0     2       1  \n",
       "163      2   4     2       1  "
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(heart_disease, test_size=0.3, random_state=42)\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split features and target\n",
    "In this function, we split target value from features. We should do this because we need this two parts seperated for fitting our model and predicting test features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_features_target(dataset):\n",
    "    data_X = dataset.drop(\"target\", axis=1)\n",
    "    data_y = dataset[\"target\"].copy()\n",
    "    return data_X, data_y\n",
    "\n",
    "train_X, train_y = split_features_target(train_set)\n",
    "test_X, test_y = split_features_target(test_set)\n",
    "# train_X.head()\n",
    "# train_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree fitting by sklearn\n",
    "__Sickit Learn__ library has _DecisionTreeClassifier_. We fit this model on our train set by _fit_ function of _DecisionTreeClassifier_ class. In this function, this model learn our data and make a decision tree of features. By this model, we can predict unseen data classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate model accuracy\n",
    "For calculating accuracy, we should count correct predicted labels and calculate percent of them. For this purpose, I use __accuracy_score__ function of __sklearn__ library. This function calculate accuracy of our model. <br>\n",
    "Accuracy of decision tree model is __73.6%__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree accuracy is:\n",
      "0.7362637362637363\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test_y_pred = decision_tree.predict(test_X)\n",
    "print(\"Decision tree accuracy is:\")\n",
    "print(accuracy_score(test_y, test_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing bagging and random forest\n",
    "Decision tree in the base format isn't useful very much. It has some improved version. <br> \n",
    "One of the technique to improve basic decision tree is using __ensemble methods__. <br>\n",
    "Two general idea for ensemble methods exists: <br>\n",
    "1. Bagging (Bootstraping + aggregating)\n",
    "2. Random forest\n",
    "#### 1. Make 5 groups with 150 random records from main dataset\n",
    "In this two techniques, we use multiple classifiers and rely on majority of them to improve our classification result. <br>\n",
    "So we have multiple classifiers and we want diffrent training set for each of them. Because of that, we split main train set to smaller groups to create dataset for each classifier. <br>\n",
    "In this part, I create 5 groups with 150 records in each. __make_one_data_group__ create each of this groups. __make_all_data_groups__ outputs all training sets for classifiers. <br>\n",
    "This part called __bootstrapping__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_data_group(dataset, records_count):\n",
    "    shuffled_indices = np.random.choice(len(data_set), records_count)\n",
    "    chosen_indices = shuffled_indices[:records_count]\n",
    "    return data_set.iloc[chosen_indices]\n",
    "\n",
    "def make_all_data_groups(dataset, group_count, records_count):\n",
    "    data_groups = []\n",
    "    for _ in range(group_count):\n",
    "        data_group = make_one_data_group(dataset, records_count)\n",
    "        data_groups.append(data_group)\n",
    "    return data_groups\n",
    "\n",
    "group_count = 5\n",
    "group_members_count = 150\n",
    "train_set, test_set = train_test_split(heart_disease, test_size=0.2, random_state=42)\n",
    "data_groups = make_all_data_groups(train_set, group_count, group_members_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implementing bagging ensemble method\n",
    "In this part, we want to create bagging ensemble model. It has two parts: __bootrapping__ and __aggregating__.\n",
    "In __botstrapping__, we prepare data groups for different classifiers. _split_data_groups_ split features and target values in data groups of classifiers. After that we make multiple classifiers and fit related data to each classifier. <br>\n",
    "In __aggregating__, the final step of bagging, we predict each data by each decision tree and label data by the majority class that predicted by classifiers. __bagging_predict__ is the main function of predict test set by bagging ensemble method. __calculate_majority__ calculates final label of each data record. <br>\n",
    "Bagging accuracy is __77.1%__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging accuracy is:\n",
      "0.7704918032786885\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "def split_data_groups(data_groups):\n",
    "    splitted_data_groups = []\n",
    "    for index in range(len(data_groups)):\n",
    "        data_group_X, data_group_y = split_features_target(data_groups[index])\n",
    "        splitted_data_groups.append({'X': data_group_X ,'y': data_group_y})\n",
    "    return splitted_data_groups\n",
    "\n",
    "def make_classifiers(training_sets):\n",
    "    classifiers = []\n",
    "    for index in range(len(training_sets)):\n",
    "        decision_tree = DecisionTreeClassifier()\n",
    "        decision_tree.fit(training_sets[index]['X'], training_sets[index]['y'])\n",
    "        classifiers.append(decision_tree)\n",
    "    return classifiers\n",
    "\n",
    "def calculate_majority(sum_prediction):\n",
    "    return [int(sum_predict>2) for sum_predict in sum_prediction]\n",
    "\n",
    "def bagging_predict(classifiers, test_X):\n",
    "    prediction = [0] * len(test_set)\n",
    "    for i in range(len(classifiers)):\n",
    "        predict_i = classifiers[i].predict(test_X)\n",
    "        prediction = list(map(add, prediction, predict_i))\n",
    "    return calculate_majority(prediction)\n",
    "\n",
    "splitted_data_groups = split_data_groups(data_groups)\n",
    "classifiers = make_classifiers(splitted_data_groups)\n",
    "test_X, test_y = split_features_target(test_set)\n",
    "test_y_pred = bagging_predict(classifiers, test_X)\n",
    "print(\"Bagging accuracy is:\")\n",
    "print(accuracy_score(test_y, test_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Calculating accuracy with deleting features\n",
    "In this part, we delete every feature once and make decision tree without that feature and calculates model accuracy to obtain the most important feature. <br>\n",
    "We see that _sex_ and _exang_ doesn't have effect on accuracy. Removing _ca_ and _oldpeak_ attributes improve accuracy of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging without age accuracy is:\n",
      "0.7540983606557377\n",
      "************************************\n",
      "Bagging without sex accuracy is:\n",
      "0.7704918032786885\n",
      "************************************\n",
      "Bagging without cp accuracy is:\n",
      "0.6885245901639344\n",
      "************************************\n",
      "Bagging without trestbps accuracy is:\n",
      "0.6885245901639344\n",
      "************************************\n",
      "Bagging without chol accuracy is:\n",
      "0.7377049180327869\n",
      "************************************\n",
      "Bagging without fbs accuracy is:\n",
      "0.7540983606557377\n",
      "************************************\n",
      "Bagging without restecg accuracy is:\n",
      "0.7213114754098361\n",
      "************************************\n",
      "Bagging without thalach accuracy is:\n",
      "0.7540983606557377\n",
      "************************************\n",
      "Bagging without exang accuracy is:\n",
      "0.7704918032786885\n",
      "************************************\n",
      "Bagging without oldpeak accuracy is:\n",
      "0.7868852459016393\n",
      "************************************\n",
      "Bagging without slope accuracy is:\n",
      "0.7540983606557377\n",
      "************************************\n",
      "Bagging without ca accuracy is:\n",
      "0.8032786885245902\n",
      "************************************\n",
      "Bagging without thal accuracy is:\n",
      "0.7377049180327869\n",
      "************************************\n"
     ]
    }
   ],
   "source": [
    "def ommit_feature_from(data_groups):\n",
    "    modified_data_groups = []\n",
    "    for data_group in data_groups:\n",
    "        modified_data_group = data_group.drop(feature, axis=1)\n",
    "        modified_data_groups.append(modified_data_group)\n",
    "    return modified_data_groups\n",
    "\n",
    "group_count = 5\n",
    "group_members_count = 150\n",
    "features = list(train_set.columns)\n",
    "features.pop(-1)\n",
    "for feature in features:\n",
    "    modified_data_groups = ommit_feature_from(data_groups)\n",
    "    splitted_data_groups = split_data_groups(modified_data_groups)\n",
    "    classifiers = make_classifiers(splitted_data_groups)\n",
    "    test_X, test_y = split_features_target(test_set)\n",
    "    test_X.drop(feature, axis=1, inplace=True)\n",
    "    test_y_pred = bagging_predict(classifiers, test_X)\n",
    "    print(\"Bagging without \" + feature + \" accuracy is:\")\n",
    "    print(accuracy_score(test_y, test_y_pred))\n",
    "    print(\"************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Choosing 5 random features and make decision tree for them\n",
    "We choose 5 random feature from features set and we make decision tree for this features. <br>\n",
    "This is pre exercise for random forest. Because random forest consists of trees like this. Each tree has some random features for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree with random features['oldpeak', 'restecg', 'ca', 'slope', 'exang', 'target']accuracy is:\n",
      "0.7377049180327869\n"
     ]
    }
   ],
   "source": [
    "def get_random_features(features, features_count):\n",
    "    shuffled_indices = np.random.permutation(len(features))\n",
    "    chosen_indices = shuffled_indices[:features_count]\n",
    "    return [features[index] for index in chosen_indices]\n",
    "\n",
    "random_features = get_random_features(features, 5)\n",
    "random_features.append('target')\n",
    "modified_train_set = train_set[random_features]\n",
    "modified_test_set = test_set[random_features]\n",
    "train_X, train_y = split_features_target(modified_train_set)\n",
    "test_X, test_y = split_features_target(modified_test_set)\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(train_X, train_y)\n",
    "test_y_pred = decision_tree.predict(test_X)\n",
    "print(\"Decision tree with random features\" + str(random_features) + \"accuracy is:\")\n",
    "print(accuracy_score(test_y, test_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Implementing random forest\n",
    "We create multiple trees, each one have 5 random feature and created by those features. <br>\n",
    "We have 5 trees. __make_classifiers__ function create this trees. <br> \n",
    "After creating trees, we predict each test data by trees and label data with majority predicted class of trees. __random_forest_predict__ play main role in this part. __calculate_majority__ function calculates final label for the record. Final accuracy of this model is __80.3%__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest accuracy is:\n",
      "0.8032786885245902\n"
     ]
    }
   ],
   "source": [
    "def make_classifiers(training_sets):\n",
    "    classifiers = []\n",
    "    selected_features = [] \n",
    "    for index in range(len(training_sets)):\n",
    "        random_features = get_random_features(features, 5)\n",
    "        modified_train_set = training_sets[index]['X'][random_features]\n",
    "        decision_tree = DecisionTreeClassifier()\n",
    "        decision_tree.fit(modified_train_set, training_sets[index]['y'])\n",
    "        classifiers.append(decision_tree)\n",
    "        selected_features.append(random_features)\n",
    "    return classifiers, selected_features\n",
    "\n",
    "def calculate_majority(sum_prediction):\n",
    "    return [int(sum_predict>2) for sum_predict in sum_prediction]\n",
    "\n",
    "def random_forest_predict(classifiers, features, test_X):\n",
    "    prediction = [0] * len(test_set)\n",
    "    for i in range(len(classifiers)):\n",
    "        random_features = features[i]\n",
    "        test_X_i = test_X[random_features]\n",
    "        predict_i = classifiers[i].predict(test_X_i)\n",
    "        prediction = list(map(add, prediction, predict_i))\n",
    "    return calculate_majority(prediction)\n",
    "\n",
    "splitted_data_groups = split_data_groups(data_groups)\n",
    "classifiers, classifier_features = make_classifiers(splitted_data_groups)\n",
    "test_X, test_y = split_features_target(test_set)\n",
    "test_y_pred = random_forest_predict(classifiers, classifier_features, test_X)\n",
    "print(\"Random forest accuracy is:\")\n",
    "print(accuracy_score(test_y, test_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions \n",
    "### 1. What is bootstrapping? What is it's effect on variance and standard deviation?\n",
    "Given a training set, we produce multiple different training sets (called bootstrap samples), by sampling with replacement from the original dataset. This process is called __bootstrapping__. <br>\n",
    "This method decreases _variance_ and _standard deviation_.\n",
    "### 2. What is overfitting and why decision tree is sensitive about it ? What is effect of bagging ? \n",
    "Overfitting is when your train model has a very high accuracy on train set. In this situation, we can say decision tree is fitted very much to train set and can't predict new unseen data. <br> \n",
    "Decision tree is very sensitive about it because if we had enough features, it's fitted right to the train set because it's learn the whole data and can guess all of data by it's decision points. Our model accuracy on train set can reach to 1. <br> \n",
    "__Bagging__ designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. It's correlated to train data less because each tree learn some parts of data and because of that chance of overfitting get decreased.\n",
    "### 3. What is relation between random forest and bagging ? What is the effect of random forest ? \n",
    "The __random forest__ approach is a __bagging method__ where deep trees, fitted on bootstrap samples, are combined to produce an output with lower variance. However, random forests also use another trick to make the multiple fitted trees a bit less correlated with each others: when growing each tree, _instead of only sampling over the observations in the dataset to generate a bootstrap sample, we also __sample over features__ and keep only a random subset of them to build the tree._ <br>\n",
    "Sampling over features has indeed the effect that all trees do not look at the exact same information to make their decisions and, so, it reduces the correlation between the different returned outputs. Another advantage of sampling over the features is that it makes the decision making process more robust to missing data: observations (from the training dataset or not) with missing data can still be regressed or classified based on the trees that take into account only features where data are not missing. Thus, random forest algorithm combines the concepts of bagging and random feature subspace selection to create more robust and more accurate models.\n",
    "### 4. What is the conclusion and relation between decision tree, bagging and random forests result? \n",
    "__Bagging__ has more accuracy than __normal decision tree__ because it use multiple classes and has less error. __Random forests__ has the most accuracy between this models because it use random different features and train variant trees to consider different features. For this reason, it has less variance and has more accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
